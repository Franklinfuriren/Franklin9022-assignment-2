{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf979b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device to accelerate the computation\n",
    "#check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4096\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(\"trainset size: \", len(trainset))\n",
    "print(\"testset size: \", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e62c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # first layer: 3 input channels, 32 output channels, 3x3 convolutions\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # second layer: 32 input channels, 64 output channels, 3x3 convolutions\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # third layer: 64 input channels, 128 output channels, 3x3 convolutions\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1 conv + ReLU + pool\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        # layer 2 conv + ReLU + pool\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        # layer 3 conv + ReLU + pool\n",
    "        x = self.pool3(torch.relu(self.conv3(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder = EncoderNet()\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one random image from the training set\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "image = images[0]\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)\n",
    "\n",
    "output = encoder(image)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774cf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderNet, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            128, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(\n",
    "            64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        # Final convolution to get 3 output channels (RGB image)\n",
    "        self.final_conv = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each deconvolution (upsampling) layer\n",
    "        x = torch.relu(self.deconv1(x))  # Upsample to (128, 8, 8)\n",
    "        x = torch.relu(self.deconv2(x))  # Upsample to (64, 16, 16)\n",
    "        x = torch.relu(self.deconv3(x))  # Upsample to (32, 32, 32)\n",
    "\n",
    "        # Apply final convolution and get RGB output\n",
    "        x = torch.sigmoid(self.final_conv(x))  # Output RGB image (3 channels)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Example latent input with 128 channels and 4x4 spatial size\n",
    "decoder = DecoderNet()\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77893ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one random image from the training set\n",
    "decoder_input = output\n",
    "\n",
    "decoder_output = decoder(decoder_input)\n",
    "print(decoder_input.shape)\n",
    "print(decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5dda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifierNet, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.flatten = nn.Flatten()  # Flatten the input\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)  # First fully connected layer\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer\n",
    "        # Final fully connected layer to output class probabilities\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten the input\n",
    "        # Apply ReLU activation after the first fully connected layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "\n",
    "classifier = ClassifierNet()\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57334a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_input = output\n",
    "classifier_output = classifier(classifier_input)\n",
    "print(classifier_input.shape)\n",
    "print(classifier_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualHeadNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderNet()  # shared encoder\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = DecoderNet()\n",
    "\n",
    "        # classifier\n",
    "        self.classifier = ClassifierNet()\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstruction = self.decoder(latent)\n",
    "        class_logits = self.classifier(latent)\n",
    "        return reconstruction, class_logits\n",
    "    \n",
    "dual_head_net = DualHeadNet()\n",
    "print(dual_head_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dual_head_net(image)\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_total_loss(recon_loss, class_loss, alpha=1.0, beta=1.0):\n",
    "    return alpha * recon_loss + beta * class_loss\n",
    "\n",
    "\n",
    "def evaluation(dataloader, model, reconstruction_criterion, classification_criterion, alpha, beta):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    total_recon_loss = 0.0\n",
    "    total_class_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forward\n",
    "            recon, logits = model(images)\n",
    "            recon_loss = reconstruction_criterion(recon, images).item()\n",
    "            class_loss = classification_criterion(logits, labels).item()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            total_samples += batch_size\n",
    "            total_recon_loss += recon_loss * batch_size\n",
    "            total_class_loss += class_loss * batch_size\n",
    "\n",
    "            # 准确率统计\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "\n",
    "    avg_recon = total_recon_loss / total_samples\n",
    "    avg_class = total_class_loss / total_samples\n",
    "    total_loss = cal_total_loss(avg_recon, avg_class, alpha, beta)\n",
    "    acc = 100. * correct / total_samples\n",
    "\n",
    "    return avg_recon, avg_class, total_loss, acc\n",
    "\n",
    "\n",
    "def train(model, dataloader, epoch, alpha, beta,  optimizer=None, recon_criterion=None, class_criterion=None):\n",
    "    model.train()\n",
    "    total_recon_loss = 0.0\n",
    "    total_class_loss = 0.0\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        batch_size = images.size(0)\n",
    "        total_samples += batch_size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        recon_images, class_logits = model(images)\n",
    "\n",
    "        # compute loss\n",
    "        loss_recon = recon_criterion(recon_images, images)\n",
    "        loss_class = class_criterion(class_logits, labels)\n",
    "        total_loss = cal_total_loss(loss_recon, loss_class, alpha, beta)\n",
    "\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # collect statistics\n",
    "        total_recon_loss += loss_recon.item()\n",
    "        total_class_loss += loss_class.item()\n",
    "        pred = class_logits.argmax(dim=1)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "\n",
    "    # calculate statistics for the epoch\n",
    "    avg_recon = total_recon_loss / total_samples\n",
    "    avg_class = total_class_loss / total_samples\n",
    "    avg_total_loss = cal_total_loss(avg_recon, avg_class, alpha, beta)\n",
    "    acc = 100. * correct / total_samples\n",
    "\n",
    "    return {\n",
    "        'total_loss': avg_total_loss,\n",
    "        'recon_loss': avg_recon,\n",
    "        'class_loss': avg_class,\n",
    "        'acc': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1578cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "recon_criterion = nn.MSELoss()         # reconstruction loss\n",
    "class_criterion = nn.CrossEntropyLoss()  # classification loss\n",
    "\n",
    "# define important constant training parameters\n",
    "NUM_EPOCHS = 1000\n",
    "EARLY_STOPPING = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha is 1.0 and Beta is 1.0\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "filename = 'best_model_a1_b1.pth'\n",
    "\n",
    "# define the model, optimizer, and scheduler\n",
    "model = DualHeadNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,     T_max=50,  # maximum number of epochs\n",
    "    eta_min=1e-5  # minimum learning rate\n",
    ")\n",
    "\n",
    "# train the model\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0  # number of epochs with no improvement\n",
    "\n",
    "epoch_pbar = tqdm.tqdm(range(NUM_EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "min_avg_total_loss = 0\n",
    "epochs = 0\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    all_train_metrics = train(model, trainloader, epoch, alpha, beta,\n",
    "                              optimizer, recon_criterion, class_criterion)\n",
    "\n",
    "    avg_recon, avg_class, total_loss, acc = evaluation(\n",
    "        testloader, model, recon_criterion, class_criterion, alpha, beta)\n",
    "\n",
    "    scheduler.step(total_loss)\n",
    "    if total_loss < best_val_loss:\n",
    "        best_val_loss = total_loss\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), filename)  # save best model\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= EARLY_STOPPING:\n",
    "            epoch_pbar.write(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(filename))\n",
    "avg_recon, avg_class, total_loss, acc = evaluation(testloader, model, recon_criterion,\n",
    "                                                   class_criterion, alpha, beta)\n",
    "\n",
    "print(f'Final => '\n",
    "      f'Avg Recon Loss: {avg_recon:.4f} | '\n",
    "      f'Avg Class Loss: {avg_class:.4f} | '\n",
    "      f'Total loss: {total_loss:.4f} | '\n",
    "      f'Class Acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha is 10.0 and Beta is 1.0\n",
    "alpha = 10.0\n",
    "beta = 1.0\n",
    "filename = 'best_model_a10_b1.pth'\n",
    "\n",
    "# define the model, optimizer, and scheduler\n",
    "model = DualHeadNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,     T_max=50,  # maximum number of epochs\n",
    "    eta_min=1e-5  # minimum learning rate\n",
    ")\n",
    "# train the model\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0  # number of epochs with no improvement\n",
    "\n",
    "epoch_pbar = tqdm.tqdm(range(NUM_EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "min_avg_total_loss = 0\n",
    "epochs = 0\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    all_train_metrics = train(model, trainloader, epoch, alpha, beta,\n",
    "                              optimizer, recon_criterion, class_criterion)\n",
    "\n",
    "    avg_recon, avg_class, total_loss, acc = evaluation(\n",
    "        testloader, model, recon_criterion, class_criterion, alpha, beta)\n",
    "\n",
    "    scheduler.step(total_loss)\n",
    "    if total_loss < best_val_loss:\n",
    "        best_val_loss = total_loss\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), filename)  # save best model\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= EARLY_STOPPING:\n",
    "            epoch_pbar.write(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(filename))\n",
    "avg_recon, avg_class, total_loss, acc = evaluation(testloader, model, recon_criterion,\n",
    "                                                   class_criterion, alpha, beta)\n",
    "\n",
    "print(f'Final => '\n",
    "      f'Avg Recon Loss: {avg_recon:.4f} | '\n",
    "      f'Avg Class Loss: {avg_class:.4f} | '\n",
    "      f'Total loss: {total_loss:.4f} | '\n",
    "      f'Class Acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha is 10.0 and Beta is 10.0\n",
    "alpha = 10.0\n",
    "beta = 10.0\n",
    "filename = 'best_model_a10_b10.pth'\n",
    "\n",
    "# define the model, optimizer, and scheduler\n",
    "model = DualHeadNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,     T_max=50,  # maximum number of epochs\n",
    "    eta_min=1e-5  # minimum learning rate\n",
    ")\n",
    "\n",
    "# train the model\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0  # number of epochs with no improvement\n",
    "\n",
    "epoch_pbar = tqdm.tqdm(range(NUM_EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "min_avg_total_loss = 0\n",
    "epochs = 0\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    all_train_metrics = train(model, trainloader, epoch, alpha, beta,\n",
    "                              optimizer, recon_criterion, class_criterion)\n",
    "\n",
    "    avg_recon, avg_class, total_loss, acc = evaluation(\n",
    "        testloader, model, recon_criterion, class_criterion, alpha, beta)\n",
    "\n",
    "    scheduler.step(total_loss)\n",
    "    if total_loss < best_val_loss:\n",
    "        best_val_loss = total_loss\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), filename)  # save best model\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= EARLY_STOPPING:\n",
    "            epoch_pbar.write(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(filename))\n",
    "avg_recon, avg_class, total_loss, acc = evaluation(testloader, model, recon_criterion,\n",
    "                                                   class_criterion, alpha, beta)\n",
    "\n",
    "print(f'Final => '\n",
    "      f'Avg Recon Loss: {avg_recon:.4f} | '\n",
    "      f'Avg Class Loss: {avg_class:.4f} | '\n",
    "      f'Total loss: {total_loss:.4f} | '\n",
    "      f'Class Acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha is 5.0 and Beta is 5.0\n",
    "alpha = 5.0\n",
    "beta = 5.0\n",
    "filename = 'best_model_a5_b5.pth'\n",
    "\n",
    "# define the model, optimizer, and scheduler\n",
    "model = DualHeadNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,     T_max=50,  # maximum number of epochs\n",
    "    eta_min=1e-5  # minimum learning rate\n",
    ")\n",
    "\n",
    "# train the model\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0  # number of epochs with no improvement\n",
    "\n",
    "epoch_pbar = tqdm.tqdm(range(NUM_EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "min_avg_total_loss = 0\n",
    "epochs = 0\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    all_train_metrics = train(model, trainloader, epoch, alpha, beta,\n",
    "                              optimizer, recon_criterion, class_criterion)\n",
    "\n",
    "    avg_recon, avg_class, total_loss, acc = evaluation(\n",
    "        testloader, model, recon_criterion, class_criterion, alpha, beta)\n",
    "\n",
    "    scheduler.step(total_loss)\n",
    "    if total_loss < best_val_loss:\n",
    "        best_val_loss = total_loss\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), filename)  # save best model\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= EARLY_STOPPING:\n",
    "            epoch_pbar.write(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(filename))\n",
    "avg_recon, avg_class, total_loss, acc = evaluation(testloader, model, recon_criterion,\n",
    "                                                   class_criterion, alpha, beta)\n",
    "\n",
    "print(f'Final => '\n",
    "      f'Avg Recon Loss: {avg_recon:.4f} | '\n",
    "      f'Avg Class Loss: {avg_class:.4f} | '\n",
    "      f'Total loss: {total_loss:.4f} | '\n",
    "      f'Class Acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d35710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha is 0.1 and Beta is 1.0\n",
    "alpha = 0.1\n",
    "beta = 1.0\n",
    "filename = 'best_model_a0.1_b1.0.pth'\n",
    "\n",
    "# define the model, optimizer, and scheduler\n",
    "model = DualHeadNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,     T_max=50,  # maximum number of epochs\n",
    "    eta_min=1e-5  # minimum learning rate\n",
    ")\n",
    "\n",
    "# train the model\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0  # number of epochs with no improvement\n",
    "\n",
    "epoch_pbar = tqdm.tqdm(range(NUM_EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "min_avg_total_loss = 0\n",
    "epochs = 0\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    all_train_metrics = train(model, trainloader, epoch, alpha, beta,\n",
    "                              optimizer, recon_criterion, class_criterion)\n",
    "\n",
    "    avg_recon, avg_class, total_loss, acc = evaluation(\n",
    "        testloader, model, recon_criterion, class_criterion, alpha, beta)\n",
    "\n",
    "    scheduler.step(total_loss)\n",
    "    if total_loss < best_val_loss:\n",
    "        best_val_loss = total_loss\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), filename)  # save best model\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= EARLY_STOPPING:\n",
    "            epoch_pbar.write(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(filename))\n",
    "avg_recon, avg_class, total_loss, acc = evaluation(testloader, model, recon_criterion,\n",
    "                                                   class_criterion, alpha, beta)\n",
    "\n",
    "print(f'Final => '\n",
    "      f'Avg Recon Loss: {avg_recon:.4f} | '\n",
    "      f'Avg Class Loss: {avg_class:.4f} | '\n",
    "      f'Total loss: {total_loss:.4f} | '\n",
    "      f'Class Acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1469e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha is 1.0 and Beta is 0.1\n",
    "alpha = 0.1\n",
    "beta = 1.0\n",
    "filename = 'best_model_a1.0_b0.1.pth'\n",
    "\n",
    "# define the model, optimizer, and scheduler\n",
    "model = DualHeadNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,     T_max=50,  # maximum number of epochs\n",
    "    eta_min=1e-5  # minimum learning rate\n",
    ")\n",
    "\n",
    "# train the model\n",
    "best_val_loss = float('inf')\n",
    "no_improve_epochs = 0  # number of epochs with no improvement\n",
    "\n",
    "epoch_pbar = tqdm.tqdm(range(NUM_EPOCHS), desc=\"Training\", unit=\"epoch\")\n",
    "min_avg_total_loss = 0\n",
    "epochs = 0\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    all_train_metrics = train(model, trainloader, epoch, alpha, beta,\n",
    "                              optimizer, recon_criterion, class_criterion)\n",
    "\n",
    "    avg_recon, avg_class, total_loss, acc = evaluation(\n",
    "        testloader, model, recon_criterion, class_criterion, alpha, beta)\n",
    "\n",
    "    scheduler.step(total_loss)\n",
    "    if total_loss < best_val_loss:\n",
    "        best_val_loss = total_loss\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), filename)  # save best model\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= EARLY_STOPPING:\n",
    "            epoch_pbar.write(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load(filename))\n",
    "avg_recon, avg_class, total_loss, acc = evaluation(testloader, model, recon_criterion,\n",
    "                                                   class_criterion, alpha, beta)\n",
    "\n",
    "print(f'Final => '\n",
    "      f'Avg Recon Loss: {avg_recon:.4f} | '\n",
    "      f'Avg Class Loss: {avg_class:.4f} | '\n",
    "      f'Total loss: {total_loss:.4f} | '\n",
    "      f'Class Acc: {acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
